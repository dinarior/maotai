system(paste(shQuote(file.path(R.home("bin"), "R")),
"CMD", "Rd2pdf", shQuote(path)))
library(maotai)
pack <- "maotai"
path <- find.package(pack)
system(paste(shQuote(file.path(R.home("bin"), "R")),
"CMD", "Rd2pdf", shQuote(path)))
library(maotai)
library(Schur)
x
library(Matrix)
x = matrix(rnorm(25),nrow=5)
Schur(x)
xx = Schur(x)
xx$Q%*5xx$T - x
xx$Q%*%xx$T - x
Q = xx$Q
QQ = xx$Q
TT = xx$T
QQ%*%TT%*%t(QQ)
QQ%*%TT%*%t(QQ)-x
usethis::use_code_of_conduct()
usethis::use_travis()
usethis::use_travis()
library(maotai)
library(maotai)
usethis::use_news_md()
usethis::use_news_md()
help(maotai)
library(maotai)
library(maotai)
library(maotai)
library(maotai)
library(maotai)
help("cmdscale")
require(graphics)
loc <- cmdscale(eurodist)
x <- loc[, 1]
y <- -loc[, 2] # reflect so North is at the top
loc
xx = eurodist
xx
xx = as.matrix(xx)
xx
dmat = xx
embedded = cmdscale(dmat, k=nrow(dmat)-1)
dmat
eigen(dmat)$values
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
H = -(J%*%dmat%*%J)/2.0
eigen(H)$values
help(eigen)
base::eigen(H, symmetric = TRUE, only.values = TRUE)
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
H = -(J%*%dmat%*%J)/2.0
kk = max(length(which(base::eigen(H, symmetric = TRUE, only.values = TRUE)$values>sqrt(.Machine$double.eps))), 2)
kk
#' @keywords internal
#' @noRd
aux_pseudomean_dim <- function(dmat){
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
H = -(J%*%dmat%*%J)/2.0
kk = max(length(which(base::eigen(H, symmetric = TRUE, only.values = TRUE)$values>sqrt(.Machine$double.eps))), 2)
return(kk)
}
# we need embedding .. umm .. dimension .. dmat = (n x n)
embedded = stats::cmdscale(dmat, k=aux_pseudomean_dim(dmat)) # embedded
dmat
eigen(dmat)$values
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
H = -(J%*%(dmat^2)%*%J)/2.0
kk = max(length(which(base::eigen(H, symmetric = TRUE, only.values = TRUE)$values>sqrt(.Machine$double.eps))), 2)
kk
kk = max(length(which(base::eigen(H, symmetric = TRUE, only.values = TRUE)$values>sqrt(.Machine$double.eps)))-1, 2)
#' @keywords internal
#' @noRd
aux_pseudomean_auto <- function(dmat){
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
H = -(J%*%(dmat^2)%*%J)/2.0
kk = max(length(which(base::eigen(H, symmetric = TRUE, only.values = TRUE)$values>sqrt(.Machine$double.eps)))-1, 2)
return(kk)
}
# we need embedding .. umm .. dimension .. dmat = (n x n)
embedded = stats::cmdscale(dmat, k=aux_pseudomean_dim(dmat)) # embedded
# we need embedding .. umm .. dimension .. dmat = (n x n)
embedded = stats::cmdscale(dmat, k=aux_pseudomean_auto(dmat)) # embedded
dim(embedded)
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
B = -(J%*%(dmat^2)%*%J)/2.0
eigB = base::eigen(B, symmetric = TRUE)
eigB$vectors
eigB$values
#' @keywords internal
#' @noRd
aux_pseudomean_auto <- function(dmat){ # only positive eigenvalues' part
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
B = -(J%*%(dmat^2)%*%J)/2.0
eigB = base::eigen(B, symmetric = TRUE) # decreasing order
m = max(length(which(eigB$values > 0)),2)
X = (eigB$vectors[,1:m])%*%(base::diag(sqrt(eigB$values[1:m])))
return(X)
}
hey = aux_pseudomean_auto(dmat)
dim(hey)
hey
plot(hey[,1],hey[,2])
hh = cmdscale(dmat, k=2)
plot(hh[,1], hh[,2])
# we need embedding .. umm .. automatic dimension selection
embedded = aux_pseudomean_auto(dmat)
# centering based on other points
emcenter = as.vector(base::colMeans(embedded[2:nrow(embedded),]))
emcenter
x = rnorm(3)
matrix(rep(x,3),ncol=3,byrow=TRUE)
xx = matrix(rep(x,3),ncol=3,byrow=TRUE)
lower.tri(xx)
xx[lower.tri(xx)]
xx
xx^2
xx*(xx^2)
xx^3
dmat
help(lower.tri)
library(maotai)
#' @keywords internal
#' @noRd
aux_pseudomean_auto <- function(dmat){ # only positive eigenvalues' part
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
B = -(J%*%(dmat^2)%*%J)/2.0
eigB = base::eigen(B, symmetric = TRUE) # decreasing order
m = max(length(which(eigB$values > 0)),2)
X = (eigB$vectors[,1:m])%*%(base::diag(sqrt(eigB$values[1:m])))
return(X)
}
# (2) aux_pseudomean ------------------------------------------------------
#' @keywords internal
#' @noRd
aux_pseudomean <- function(dmat){
# we need embedding .. umm .. automatic dimension selection
embedded = aux_pseudomean_auto(dmat)
n = nrow(embedded)
p = ncol(embedded)
# centering based on other points
emcenter = as.vector(base::colMeans(embedded[2:n,]))
embednew = embedded - matrix(rep(emcenter,n), ncol=p, byrow=TRUE)
# compute scalar
d1mat = dmat[2:n,2:n]                          # d(x,y)
d2mat = as.matrix(stats::dist(embednew[2:n,])) # ||x-y||
d12mat = (d1mat*d2mat)
d22mat = (d2mat^2)
dlower = base::lower.tri(d12mat)
cstar =sum(d12mat[dlower])/sum(d22mat[dlower])
# update embednew and compute
erow1 = c*as.vector(embednew[1,])
return(sqrt(sum(erow1^2)))
}
x = rnorm(5, mean=3)
y = matrix(rnorm(10*5),ncol=5)
dmat = as.matrix(dist(rbind(x,y)))
aux_pseudomean(dmat)
# we need embedding .. umm .. automatic dimension selection
embedded = aux_pseudomean_auto(dmat)
embedded
# we need embedding .. umm .. automatic dimension selection
embedded = aux_pseudomean_auto(dmat)
n = nrow(embedded)
p = ncol(embedded)
# centering based on other points
emcenter = as.vector(base::colMeans(embedded[2:n,]))
embednew = embedded - matrix(rep(emcenter,n), ncol=p, byrow=TRUE)
# compute scalar
d1mat = dmat[2:n,2:n]                          # d(x,y)
d2mat = as.matrix(stats::dist(embednew[2:n,])) # ||x-y||
d12mat = (d1mat*d2mat)
d22mat = (d2mat^2)
dlower = base::lower.tri(d12mat)
cstar =sum(d12mat[dlower])/sum(d22mat[dlower])
# update embednew and compute
erow1 = c*as.vector(embednew[1,])
embednew
# update embednew and compute
erow1 = cstar*as.vector(embednew[1,])
return(sqrt(sum(erow1^2)))
dlower = base::lower.tri(d12mat)
cstar =sum(d12mat[dlower])/sum(d22mat[dlower])
# update embednew and compute
erow1 = cstar*as.vector(embednew[1,])
sqrt(sum(erow1^2))
sqrt(sum(x^2))
rm(list=ls())
# personal test
x = rnorm(5, mean=3)
y = matrix(rnorm(10*5),ncol=5)
dmat = as.matrix(dist(rbind(x,y)))
val.alg = aux_pseudomean(dmat)
#' @keywords internal
#' @noRd
aux_pseudomean_auto <- function(dmat){ # only positive eigenvalues' part
n = nrow(dmat)
J = diag(rep(1,n))-(1/n)*outer(rep(1,n),rep(1,n))
B = -(J%*%(dmat^2)%*%J)/2.0
eigB = base::eigen(B, symmetric = TRUE) # decreasing order
m = max(length(which(eigB$values > 0)),2)
X = (eigB$vectors[,1:m])%*%(base::diag(sqrt(eigB$values[1:m])))
return(X)
}
# (2) aux_pseudomean ------------------------------------------------------
#' @keywords internal
#' @noRd
aux_pseudomean <- function(dmat){
# we need embedding .. umm .. automatic dimension selection
embedded = aux_pseudomean_auto(dmat)
n = nrow(embedded)
p = ncol(embedded)
# centering based on other points
emcenter = as.vector(base::colMeans(embedded[2:n,]))
embednew = embedded - matrix(rep(emcenter,n), ncol=p, byrow=TRUE)
# compute scalar
d1mat = dmat[2:n,2:n]                          # d(x,y)
d2mat = as.matrix(stats::dist(embednew[2:n,])) # ||x-y||
d12mat = (d1mat*d2mat)
d22mat = (d2mat^2)
dlower = base::lower.tri(d12mat)
cstar =sum(d12mat[dlower])/sum(d22mat[dlower])
# update embednew and compute
erow1 = cstar*as.vector(embednew[1,])
return(sqrt(sum(erow1^2)))
}
x = rnorm(5, mean=3)
y = matrix(rnorm(10*5),ncol=5)
dmat = as.matrix(dist(rbind(x,y)))
val.alg = aux_pseudomean(dmat)
val.true = sqrt(sum((x-as.vector(colMeans(y)))^2))
val.alg
val.true
rm(list=ls())
library(maotai)
library(maotai)
library(maotai)
library(maotai)
library(maotai)
library(maotai)
usethis::use_cran_badge()
library(maotai)
library(kernlab)
help("kernelMatrix")
data(spam)
dt <- as.matrix(spam[c(10:20,3000:3010),-58])
## initialize kernel function
rbf <- rbfdot(sigma = 0.05)
rbf
## calculate kernel matrix
xx = kernelMatrix(rbf, dt)
xy = as.matrix(xx)
dim(xy)
eigen(xy)$values
class(xx)
library(maotai)
dat1 <- matrix(rnorm(60, mean= 1), ncol=2) # group 1 : 30 obs of mean  1
dat2 <- matrix(rnorm(50, mean=-1), ncol=2) # group 2 : 25 obs of mean -1
dmat <- as.matrix(dist(rbind(dat1, dat2)))  # Euclidean distance matrix
kmat <- exp(-(dmat^2))                      # build a kernel matrix
lab  <- c(rep(1,30), rep(2,25))             # corresponding label
test2.2014bg(kmat, lab)                         # run the code !
mmd2test(kmat, lab)                         # run the code !
## WARNING: computationally heavy.
#  Let's compute empirical Type 1 error at alpha=0.05
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
pvals[i] = mmd2test(exp(-as.matrix(dist(dat))^2), lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
print(paste("* Empirical Type 1 Error : ",sum(pvals<=0.05)/niter,sep=""))
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors = rep(0,100)
for (i in 1:100){
errors[i] = sum(pvals<=alphas[i])/niter
}
plot(alphas, errors, "b", main="Empirical Type 1 Errors",
xlab="alpha", ylab="error", lwd=2)
abline(v=0.05, lwd=2, col="red")
abline(a=1,b=0, lwd=2, col="red")
abline(a=0,b=1, lwd=2, col="red")
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
pvals[i] = mmd2test(exp(-(as.matrix(dist(dat))^2)*0.1), lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
print(paste("* Empirical Type 1 Error : ",sum(pvals<=0.05)/niter,sep=""))
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors = rep(0,100)
for (i in 1:100){
errors[i] = sum(pvals<=alphas[i])/niter
}
plot(alphas, errors, "b", main="Empirical Type 1 Errors",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=2, col="red")
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
pvals[i] = mmd2test(exp(-(as.matrix(dist(dat))^2)*0.1), lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
print(paste("* Empirical Type 1 Error : ",sum(pvals<=0.05)/niter,sep=""))
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors = rep(0,100)
for (i in 1:100){
errors[i] = sum(pvals<=alphas[i])/niter
}
plot(alphas, errors, "b", main="Empirical Type 1 Errors",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=2, col="red")
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
lbd = 0.1
kmat = exp(-lbd*(as.matrix(dist(dat))^2))
pvals[i] = mmd2test(kmat, lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
print(paste("* Empirical Type 1 Error : ",sum(pvals<=0.05)/niter,sep=""))
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors = rep(0,100)
for (i in 1:100){
errors[i] = sum(pvals<=alphas[i])/niter
}
plot(alphas, errors, "b", main="Empirical Type 1 Errors",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=2, col="red")
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
lbd = 0.1
kmat = exp(-lbd*(as.matrix(dist(dat))^2))
pvals[i] = mmd2test(kmat, lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
print(paste("* Empirical Type 1 Error : ",sum(pvals<=0.05)/niter,sep=""))
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors = rep(0,100)
for (i in 1:100){
errors[i] = sum(pvals<=alphas[i])/niter
}
plot(alphas, errors, "b", main="Empirical Type 1 Errors",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=2, col="red")
dat1 <- matrix(rnorm(60, mean= 1), ncol=2) # group 1 : 30 obs of mean  1
dat2 <- matrix(rnorm(50, mean=-1), ncol=2) # group 2 : 25 obs of mean -1
dmat <- as.matrix(dist(rbind(dat1, dat2)))  # Euclidean distance matrix
kmat <- exp(-(dmat^2))                      # build a gaussian kernel matrix
lab  <- c(rep(1,30), rep(2,25))             # corresponding label
mmd2test(kmat, lab)                         # run the code !
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
lbd = 0.1
kmat = exp(-lbd*(as.matrix(dist(dat))^2))
pvals[i] = mmd2test(kmat, lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
kmat
eigen(kmat)$values
.Machine$double.eps
library(maotai)
library(maotai)
niter = 496
pvals = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
lbd = 0.1
kmat = exp(-lbd*(as.matrix(dist(dat))^2))
pvals[i] = mmd2test(kmat, lab)$p.value
print(paste("iteration ",i," complete..",sep=""))
}
print(paste("* Empirical Type 1 Error : ",sum(pvals<=0.05)/niter,sep=""))
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors = rep(0,100)
for (i in 1:100){
errors[i] = sum(pvals<=alphas[i])/niter
}
plot(alphas, errors, "b", main="Empirical Type 1 Errors",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=1.5, col="red")
warnings()
warnings()
round(-6.65782643941676e-15, 3)
## WARNING: computationally heavy.
#  Let's compute empirical Type 1 error at alpha=0.05
niter = 496
pvals1 = rep(0,niter)
pvals2 = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
lbd = 0.1
kmat = exp(-lbd*(as.matrix(dist(dat))^2))
pvals1[i] = mmd2test(kmat, lab, method="b")$p.value
pvals2[i] = mmd2test(kmat, lab, method="u")$p.value
print(paste("iteration ",i," complete..",sep=""))
}
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors1 = rep(0,100)
for (i in 1:100){
errors1[i] = sum(pvals1<=alphas[i])/niter
errors2[i] = sum(pvals2<=alphas[i])/niter
}
opar <- par(mfrow=c(1,2), pty="s")
plot(alphas, errors1, "b", main="Biased Estimator Type 1",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=1.5, col="red")
plot(alphas, errors2, "b", main="Unbiased Estimator Type 1",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=1.5, col="blue")
par(opar)
alphas = seq(from=0.001, to=0.999, length.out=100)
errors1 = rep(0,100)
errors2 = rep(0,100)
for (i in 1:100){
errors1[i] = sum(pvals1<=alphas[i])/niter
errors2[i] = sum(pvals2<=alphas[i])/niter
}
opar <- par(mfrow=c(1,2), pty="s")
plot(alphas, errors1, "b", main="Biased Estimator Type 1",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=1.5, col="red")
plot(alphas, errors2, "b", main="Unbiased Estimator Type 1",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=1.5, col="blue")
par(opar)
opar <- par(mfrow=c(1,2), pty="s")
plot(alphas, errors1, "b", main="Biased Estimator Error",
xlab="alpha", ylab="error", lwd=2, cex=0.5)
abline(a=0,b=1, lwd=1.5, col="red")
plot(alphas, errors2, "b", main="Unbiased Estimator Error",
xlab="alpha", ylab="error", lwd=2)
abline(a=0,b=1, lwd=1.5, col="blue")
par(opar)
plot(alphas, errors1, "b", main="Biased Estimator Error",
xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="red")
opar <- par(mfrow=c(1,2), pty="s")
plot(alphas, errors1, "b", main="Biased Estimator Error",
xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="red")
plot(alphas, errors2, "b", main="Unbiased Estimator Error",
xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="blue")
par(opar)
library(maotai)
library(maotai)
#  Let's compute empirical Type 1 error at alpha=0.05
niter = 496
pvals1 = rep(0,niter)
pvals2 = rep(0,niter)
for (i in 1:niter){
dat = matrix(rnorm(200),ncol=2)
lab = c(rep(1,50), rep(2,50))
lbd = 0.1
kmat = exp(-lbd*(as.matrix(dist(dat))^2))
pvals1[i] = mmd2test(kmat, lab, method="b")$p.value
pvals2[i] = mmd2test(kmat, lab, method="u")$p.value
print(paste("iteration ",i," complete..",sep=""))
}
#  Visualize the above at multiple significance levels
alphas = seq(from=0.001, to=0.999, length.out=100)
errors1 = rep(0,100)
errors2 = rep(0,100)
for (i in 1:100){
errors1[i] = sum(pvals1<=alphas[i])/niter
errors2[i] = sum(pvals2<=alphas[i])/niter
}
opar <- par(mfrow=c(1,2), pty="s")
plot(alphas, errors1, "b", main="Biased Estimator Error",
xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="red")
plot(alphas, errors2, "b", main="Unbiased Estimator Error",
xlab="alpha", ylab="error", cex=0.5)
abline(a=0,b=1, lwd=1.5, col="blue")
par(opar)
library(maotai)
install.packages("phangorn", dependencies = TRUE)
